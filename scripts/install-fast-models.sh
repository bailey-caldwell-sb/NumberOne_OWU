#!/bin/bash

# ğŸš€ Install Fast Ollama Models Script
# This script installs the 3 fastest local Ollama models for lightning-fast responses

set -e

echo "ğŸš€ Installing the 3 Fastest Ollama Models..."
echo "=============================================="

# Check if Ollama is installed and running
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama is not installed or not in PATH"
    echo "Please install Ollama first: https://ollama.ai"
    exit 1
fi

# Check if Ollama service is running
if ! ollama list &> /dev/null; then
    echo "âŒ Ollama service is not running"
    echo "Please start Ollama service first"
    exit 1
fi

echo "âœ… Ollama is installed and running"
echo ""

# Function to install model with progress
install_model() {
    local model=$1
    local description=$2
    local size=$3
    
    echo "ğŸ“¦ Installing $model ($description - $size)..."
    if ollama pull "$model"; then
        echo "âœ… Successfully installed $model"
    else
        echo "âŒ Failed to install $model"
        return 1
    fi
    echo ""
}

# Install the 3 fastest models
echo "Installing ultra-fast models for instant responses..."
echo ""

# 1. Qwen2.5 0.5B - Smallest and fastest
install_model "qwen2.5:0.5b" "Ultra Fast" "397 MB"

# 2. TinyLlama - Very fast and efficient
install_model "tinyllama" "Very Fast" "637 MB"

# 3. Llama3.2 1B - Best balance of speed and quality
install_model "llama3.2:1b" "Fast & Quality" "1.3 GB"

echo "ğŸ‰ All fast models installed successfully!"
echo ""

# Test the models
echo "ğŸ§ª Testing model response times..."
echo "=================================="

test_model() {
    local model=$1
    local description=$2
    
    echo "Testing $model ($description)..."
    start_time=$(date +%s.%N)
    
    if ollama run "$model" "Say hello in one sentence" >/dev/null 2>&1; then
        end_time=$(date +%s.%N)
        duration=$(echo "$end_time - $start_time" | bc -l)
        printf "âœ… %s: %.2f seconds\n" "$description" "$duration"
    else
        echo "âŒ $description: Test failed"
    fi
}

# Test all models
test_model "qwen2.5:0.5b" "Qwen2.5 0.5B"
test_model "tinyllama" "TinyLlama"
test_model "llama3.2:1b" "Llama3.2 1B"

echo ""
echo "ğŸ“Š Model Summary:"
echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
echo "â”‚ Model           â”‚ Size     â”‚ Best Use Case               â”‚"
echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
echo "â”‚ qwen2.5:0.5b    â”‚ 397 MB   â”‚ Instant responses           â”‚"
echo "â”‚ tinyllama       â”‚ 637 MB   â”‚ Simple conversations        â”‚"
echo "â”‚ llama3.2:1b     â”‚ 1.3 GB   â”‚ Best speed/quality balance  â”‚"
echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"

echo ""
echo "ğŸ¯ Usage Tips:"
echo "â€¢ Use qwen2.5:0.5b for quick questions and instant responses"
echo "â€¢ Use tinyllama for simple tasks and rapid prototyping"
echo "â€¢ Use llama3.2:1b for the best balance of speed and quality"
echo ""
echo "ğŸš€ All models are now available in Open WebUI!"
echo "Switch between them in the model selector for different use cases."
echo ""
echo "âœ¨ Installation complete! Enjoy lightning-fast AI responses! âš¡"
